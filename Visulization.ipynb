{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "from preprocessing_util import preposs_toekn, Lang, text2index, construct_Lang\n",
    "from Multilayers_Encoder import EncoderRNN\n",
    "from Multilayers_Decoder import DecoderRNN, DecoderAtten\n",
    "#from config import device, PAD_token, SOS_token, EOS_token, UNK_token, embedding_freeze, vocab_prefix\n",
    "import random\n",
    "from evaluation import evaluate_batch, evaluate_beam_batch\n",
    "import pickle \n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_address = '/scratch/tw1682/NLP/Model/vi-en-12-linear-dot/epoch_9.pth'\n",
    "config_address = '/scratch/tw1682/NLP/Model/vi-en-12-linear-dot/model_params.pkl'\n",
    "transtype = ('vi', 'en')\n",
    "check_point_state = torch.load(model_address)\n",
    "with open(config_address, 'rb') as f:\n",
    "    paras = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src_max_vocab_size': 26109, 'tgt_max_vocab_size': 24418, 'tgt_max_len': 100, 'max_src_len_dataloader': 72, 'max_tgt_len_dataloader': 71, 'emb_size': 300, 'hidden_size': 256, 'num_layers': 1, 'num_direction': 2, 'deal_bi': 'linear', 'attention_type': 'dot_prod', 'teacher_forcing_ratio': 1, 'learning_rate': 0.001, 'num_epochs': 50, 'batch_size': 100, 'beam_size': 1, 'dropout_rate': 0.1, 'model_save_info': {'model_path': 'nmt_models/vi-en-12-linear-dot/', 'epochs_per_save_model': 10, 'model_path_for_resume': None}, 'address_book': {'train_src': '/home/tw1682/Machine_Translation_NLP/iwsltzhen/iwslt-vi-en/train.tok.vi', 'train_tgt': '/home/tw1682/Machine_Translation_NLP/iwsltzhen/iwslt-vi-en/train.tok.en', 'val_src': '/home/tw1682/Machine_Translation_NLP/iwsltzhen/iwslt-vi-en/dev.tok.vi', 'val_tgt': '/home/tw1682/Machine_Translation_NLP/iwsltzhen/iwslt-vi-en/dev.tok.en', 'src_emb': '/scratch/tw1682/embedding/wiki.vi.vec', 'tgt_emb': '/scratch/tw1682/embedding/wiki.en.vec'}}\n",
      "The number of train samples:  133317\n",
      "The number of val samples:  1268\n",
      "The number of unique tokens totally in train data:  42148\n",
      "The number of unique tokens totally in train data:  54169\n",
      "finish\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "src_max_vocab_size = paras['src_max_vocab_size']\n",
    "tgt_max_vocab_size = paras['tgt_max_vocab_size']\n",
    "tgt_max_len = paras['tgt_max_len']\n",
    "max_src_len_dataloader = paras['max_src_len_dataloader']\n",
    "max_tgt_len_dataloader = paras['max_tgt_len_dataloader']\n",
    "\n",
    "teacher_forcing_ratio = paras['teacher_forcing_ratio']\n",
    "emb_size = paras['emb_size']\n",
    "hidden_size = paras['hidden_size']\n",
    "num_layers = paras['num_layers']\n",
    "num_direction = paras['num_direction']\n",
    "deal_bi = paras['deal_bi']\n",
    "learning_rate = paras['learning_rate']\n",
    "num_epochs = paras['num_epochs']\n",
    "batch_size = paras['batch_size']\n",
    "attention_type = paras['attention_type']\n",
    "beam_size = paras['beam_size']\n",
    "model_save_info = paras['model_save_info']\n",
    "dropout_rate = paras['dropout_rate']\n",
    "\n",
    "# address_book=dict(\n",
    "#     train_src = 'Machine_Translation_NLP/iwsltzhen/iwslt-{}-{}/train.tok.{}'.format(transtype[0], transtype[1], transtype[0]),\n",
    "#     train_tgt = 'Machine_Translation_NLP/iwsltzhen/iwslt-{}-{}/train.tok.{}'.format(transtype[0], transtype[1], transtype[1]),\n",
    "#     val_src = 'Machine_Translation_NLP/iwsltzhen/iwslt-{}-{}/dev.tok.{}'.format(transtype[0], transtype[1], transtype[0]),\n",
    "#     val_tgt = 'Machine_Translation_NLP/iwsltzhen/iwslt-{}-{}/dev.tok.{}'.format(transtype[0], transtype[1], transtype[1]),\n",
    "#     src_emb = 'embedding/wiki.{}.vec'.format(transtype[0]),\n",
    "#     tgt_emb = 'embedding/wiki.{}.vec'.format(transtype[1])\n",
    "#     )\n",
    "#print(address_book)\n",
    "train_src_add = address_book['train_src']\n",
    "train_tgt_add = address_book['train_tgt']\n",
    "val_src_add = address_book['val_src']\n",
    "val_tgt_add = address_book['val_tgt']\n",
    "\n",
    "# make dir for saving models\n",
    "if not os.path.exists(model_save_info['model_path']):\n",
    "    os.makedirs(model_save_info['model_path'])\n",
    "### save model hyperparameters\n",
    "with open(model_save_info['model_path']+'model_params.pkl', 'wb') as f:\n",
    "    model_hyparams = paras\n",
    "    model_hyparams['address_book'] = address_book\n",
    "    pickle.dump(model_hyparams, f)\n",
    "print(model_hyparams)\n",
    "\n",
    "train_src = []\n",
    "with open(train_src_add) as f:\n",
    "    for line in f:\n",
    "        train_src.append(preposs_toekn(line[:-1].strip().split(' ')))\n",
    "\n",
    "train_tgt = []\n",
    "with open(train_tgt_add) as f:\n",
    "    for line in f:\n",
    "        train_tgt.append(preposs_toekn(line[:-1].strip().split(' ')))\n",
    "\n",
    "val_src = []\n",
    "with open(val_src_add) as f:\n",
    "    for line in f:\n",
    "        val_src.append(preposs_toekn(line[:-1].strip().split(' ')))\n",
    "\n",
    "val_tgt = []\n",
    "with open(val_tgt_add) as f:\n",
    "    for line in f:\n",
    "        val_tgt.append(preposs_toekn(line[:-1].strip().split(' ')))\n",
    "\n",
    "print('The number of train samples: ', len(train_src))\n",
    "print('The number of val samples: ', len(val_src))\n",
    "srcLang = construct_Lang('src', src_max_vocab_size, address_book['src_emb'], train_src)\n",
    "tgtLang = construct_Lang('tgt', tgt_max_vocab_size, address_book['tgt_emb'], train_tgt)\n",
    "#train_input_index = text2index(train_src, srcLang.word2index) #add EOS token here \n",
    "#train_output_index = text2index(train_tgt, tgtLang.word2index)\n",
    "val_input_index = text2index(val_src, srcLang.word2index)\n",
    "val_output_index = text2index(val_tgt, tgtLang.word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot_prod\n"
     ]
    }
   ],
   "source": [
    "if paras[\"attention_type\"]:\n",
    "    encoder = EncoderRNN(srcLang.vocab_size, emb_size, hidden_size, num_layers, num_direction, deal_bi, embedding_weight = None, dropout_rate = dropout_rate)\n",
    "    decoder = DecoderAtten(emb_size, hidden_size, tgtLang.vocab_size, num_layers, embedding_weight = None, atten_type = attention_type, dropout_rate = dropout_rate)\n",
    "else:      \n",
    "    encoder = EncoderRNN(srcLang.vocab_size, emb_size,hidden_size, num_layers, num_direction, deal_bi, embedding_weight = None, dropout_rate = dropout_rate)\n",
    "    decoder = DecoderRNN(emb_size, hidden_size, tgtLang.vocab_size, num_layers, embedding_weight = None, dropout_rate = dropout_rate)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.load_state_dict(check_point_state['encoder_state_dict'])\n",
    "decoder.load_state_dict(check_point_state['decoder_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data_utils import VocabDataset, vocab_collate_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = VocabDataset(val_input_index,val_output_index, None, None)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                        batch_size=1,\n",
    "                                        collate_fn=vocab_collate_func,\n",
    "                                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder, decoder = encoder.to(device), decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.NLLLoss()\n",
    "# val_bleu_sacre, val_bleu_nltk, val_loss = evaluate_batch(val_loader, encoder, decoder, criterion, tgt_max_len, tgtLang.index2word, srcLang.index2word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacrebleu\n",
    "import beam\n",
    "def fun_index2token(index_list, idx2words):\n",
    "    token_list = []\n",
    "    for index in index_list:\n",
    "        if index == EOS_token:\n",
    "            break\n",
    "        else:\n",
    "            token_list.append(idx2words[index])\n",
    "    return token_list\n",
    "\n",
    "def visulization(loader, encoder, decoder, tgt_max_length, tgt_idx2words, src_idx2words):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")    \n",
    "    tgt_sents_sacre = []\n",
    "    tgt_pred_sents_sacre = []\n",
    "    src_sents = []\n",
    "    atten_weight_list = []\n",
    "    true_prob_list = []\n",
    "    with torch.no_grad():\n",
    "        for input_tensor, input_lengths, target_tensor, target_lengths in loader:\n",
    "            atten_weights = []\n",
    "            true_prob = []\n",
    "            batch_size = input_tensor.size(0)\n",
    "            encoder_hidden = encoder.initHidden(batch_size)\n",
    "            encoder_outputs, encoder_hidden = encoder(input_tensor, encoder_hidden, input_lengths)\n",
    "            decoder_input = torch.tensor([[SOS_token]*batch_size], device=device).transpose(0,1)\n",
    "            decoder_hidden = encoder_hidden\n",
    "            decoding_token_index = 0\n",
    "            target_lengths_numpy = target_lengths.cpu().numpy()\n",
    "            idx_token_pred = np.zeros((batch_size, tgt_max_length), dtype=np.int)\n",
    "            while decoding_token_index < tgt_max_length:\n",
    "                decoder_output, decoder_hidden, weight = decoder(decoder_input, decoder_hidden, input_lengths, encoder_outputs)  \n",
    "                if decoding_token_index < target_tensor.size(1):\n",
    "                    true_index = target_tensor[0,decoding_token_index].cpu().item()\n",
    "                    true_prob.append(decoder_output[0,true_index])\n",
    "                atten_weights.append(np.array(weight.squeeze(0).squeeze(0)))\n",
    "                topv, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.detach()  # detach from history as input\n",
    "                idx_token_pred_step = decoder_input.cpu().squeeze(1).numpy()\n",
    "                idx_token_pred[:, decoding_token_index] = idx_token_pred_step\n",
    "                decoding_token_index += 1\n",
    "                end_or_not = idx_token_pred_step != EOS_token\n",
    "                sent_not_end_index = list(np.where(end_or_not)[0])\n",
    "                if len(sent_not_end_index) == 0:\n",
    "                    break\n",
    "            true_prob_list.append(true_prob)\n",
    "            atten_weight_list.append(atten_weights)\n",
    "            target_tensor_numpy = target_tensor.cpu().numpy()\n",
    "            input_tensor_numpy = input_tensor.cpu().numpy()\n",
    "            for i_batch in range(batch_size):\n",
    "                tgt_sent_tokens = fun_index2token(target_tensor_numpy[i_batch].tolist(), tgt_idx2words) #:target_lengths_numpy[i_batch]\n",
    "                tgt_sents_sacre.append(' '.join(tgt_sent_tokens))\n",
    "                tgt_pred_sent_tokens = fun_index2token(idx_token_pred[i_batch].tolist(), tgt_idx2words)\n",
    "                tgt_pred_sents_sacre.append(' '.join(tgt_pred_sent_tokens))\n",
    "                src_sent_tokens = fun_index2token(input_tensor_numpy[i_batch].tolist(), src_idx2words)\n",
    "                src_sents.append(' '.join(src_sent_tokens))\n",
    "    sacre_bleu_score = sacrebleu.corpus_bleu(tgt_pred_sents_sacre, [tgt_sents_sacre], smooth='exp', smooth_floor=0.0, force=False, lowercase=False,\n",
    "        tokenize='none', use_effective_order=True)\n",
    "    if True:\n",
    "        random_sample = np.random.randint(len(tgt_pred_sents_sacre))\n",
    "        print('src:', src_sents[random_sample])\n",
    "        print('Ref: ', tgt_sents_sacre[random_sample])\n",
    "        print('pred: ', tgt_pred_sents_sacre[random_sample])\n",
    "        print('sacre_bleu_score',sacre_bleu_score)\n",
    "    return src_sents, tgt_sents_sacre, tgt_pred_sents_sacre, atten_weight_list,true_prob_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src: Nó nằm trên một dải đất mà chúng_tôi gọi_là đường đi_dạo .\n",
      "Ref:  It was on a strip of land that we call a <UNK> .\n",
      "pred:  It &apos;s on a valley of the ground we &apos;re calling a <UNK> .\n",
      "sacre_bleu_score BLEU(score=21.88761539707461, counts=[15032, 7317, 3892, 2120], totals=[27018, 25750, 24482, 23216], precisions=[55.63698275223925, 28.415533980582524, 15.897394003757864, 9.131633356305995], bp=1.0, sys_len=27018, ref_len=26728)\n"
     ]
    }
   ],
   "source": [
    "src_sents, tgt_sents_sacre, tgt_pred_sents_sacre, atten_weight_list,true_prob_list = visulization(val_loader, encoder, decoder, tgt_max_len, tgtLang.index2word, srcLang.index2word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa = np.array(atten_weight_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34, 31)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aaa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2b3978ce9e48>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b393ad1e2b0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(aaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(src_sents[0].split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tgt_sents_sacre[0].split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samuel 16 . I <UNK> . I &apos;m a very good fortune . I &apos;m a great kid .\n",
      "Samuel is 16 . He &apos;s tall . He &apos;s very handsome .\n",
      "\n",
      "I have a great memory .\n",
      "He has the most <UNK> memory .\n",
      "\n",
      "But it &apos;s selection .\n",
      "He has a selective one , though .\n",
      "\n",
      "I don &apos;t remember if I had <UNK> goggles -- my uncles , not , but remember the year &apos; <UNK> songs , the <UNK> between us from her home , my favorite talks from her home , the first <UNK> to the <UNK> , and the birthday of the OED <UNK> <UNK> .\n",
      "He doesn &apos;t remember if he stole my chocolate bar , but he remembers the year of release for every song on my iPod , conversations we had when he was four , <UNK> on my arm on the first ever episode of <UNK> , and Lady Gaga &apos;s birthday .\n",
      "\n",
      "Now , this sounds great , right ?\n",
      "Don &apos;t they sound incredible ?\n",
      "\n",
      "But most people don &apos;t agree .\n",
      "But most people don &apos;t agree .\n",
      "\n",
      "And actually because of my mind , I wasn &apos;t suited to the normal , but they often get out and wrong .\n",
      "And in fact , because their minds don &apos;t fit into society &apos;s version of normal , they &apos;re often bypassed and misunderstood .\n",
      "\n",
      "But what makes my heart sweat is more powerful and my spirit , rather more powerful than that , although this is true , whether they don &apos;t be normal , it can be the case that they &apos;re amazing -- autistic and amazing people .\n",
      "But what lifted my heart and strengthened my soul was that even though this was the case , although they were not seen as ordinary , this could only mean one thing : that they were extraordinary -- autistic and extraordinary .\n",
      "\n",
      "For those of you not familiar with the word , &quot; autistic , &quot; is a disorder of the brain that affects society , learning and sometimes practical and experiences of physical and physical and physical and more <UNK> periods .\n",
      "Now , for you who may be less familiar with the term &quot; autism , &quot; it &apos;s a complex brain disorder that affects social communication , learning and sometimes physical skills .\n",
      "\n",
      "The disease and every one of the other one , that &apos;s why <UNK> is so different Sam .\n",
      "It manifests in each individual differently , hence why <UNK> is so different from Sam .\n",
      "\n",
      "And on the entire world , for 20 minutes a person who was diagnosed with autism , and although it was one of the biggest catastrophes of growing in the world , we didn &apos;t know why or treatment .\n",
      "And across the world , every 20 minutes , one new person is diagnosed with autism , and although it &apos;s one of the fastest-growing developmental disorders in the world , there is no known cause or cure .\n",
      "\n",
      "And I can &apos;t remember the first time I got to autism , but I can &apos;t remember one day without it .\n",
      "And I cannot remember the first moment I encountered autism , but I cannot recall a day without it .\n",
      "\n",
      "I was three years old when I was born and I was fascinated to obtain a new person in my life .\n",
      "I was just three years old when my brother came along , and I was so excited that I had a new being in my life .\n",
      "\n",
      "And then a few months later , I realized that I was different .\n",
      "And after a few months went by , I realized that he was different .\n",
      "\n",
      "She &apos;s been yelling so much .\n",
      "He screamed a lot .\n",
      "\n",
      "I don &apos;t want to play , like the other fireflies , and in fact , I don &apos;t seem to be interested in me or something else .\n",
      "He didn &apos;t want to play like the other babies did , and in fact , he didn &apos;t seem very interested in me whatsoever .\n",
      "\n",
      "The <UNK> lives and <UNK> in his own world , with their own rules , and I see joy in the smallest , like <UNK> around the room and stare at the washing machine and eat whatever it is in your hand .\n",
      "<UNK> lived and <UNK> in his own world , with his own rules , and he found pleasure in the smallest things , like lining up cars around the room and staring at the washing machine and eating anything that came in between .\n",
      "\n",
      "When I grew up , I got a different , and a different clarity to become clearer .\n",
      "And as he grew older , he grew more different , and the differences became more obvious .\n",
      "\n",
      "But out of the <UNK> and <UNK> and <UNK> <UNK> , which is something really unique : a <UNK> instinct : <UNK> and naive , a little boy that sees the word stereotypes , a human being never lying .\n",
      "Yet beyond the <UNK> and the frustration and the never-ending <UNK> was something really unique : a pure and innocent nature , a boy who saw the world without prejudice , a human who had never lied .\n",
      "\n",
      "<UNK> .\n",
      "Extraordinary .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(100,120):\n",
    "    print(tgt_pred_sents_sacre[i])\n",
    "    print(tgt_sents_sacre[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tgt_sents_sacre[1].split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(-0.0602, device='cuda:0'),\n",
       " tensor(-1.6541, device='cuda:0'),\n",
       " tensor(-0.8539, device='cuda:0'),\n",
       " tensor(-0.4758, device='cuda:0'),\n",
       " tensor(-0.0029, device='cuda:0'),\n",
       " tensor(-7.1340, device='cuda:0'),\n",
       " tensor(-9.4733, device='cuda:0'),\n",
       " tensor(-10.1679, device='cuda:0'),\n",
       " tensor(-13.8375, device='cuda:0'),\n",
       " tensor(-13.8696, device='cuda:0'),\n",
       " tensor(-4.9662, device='cuda:0'),\n",
       " tensor(-13.0475, device='cuda:0'),\n",
       " tensor(-13.3779, device='cuda:0'),\n",
       " tensor(-14.5309, device='cuda:0'),\n",
       " tensor(-14.4447, device='cuda:0')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_prob_list[113]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tgt_pred_sents_sacre[0].split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(\n",
    "        encoder1, attn_decoder1, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions)\n",
    "\n",
    "\n",
    "evaluateAndShowAttention(\"elle a cinq ans de moins que moi .\")\n",
    "\n",
    "evaluateAndShowAttention(\"elle est trop petit .\")\n",
    "\n",
    "evaluateAndShowAttention(\"je ne crains pas de mourir .\")\n",
    "\n",
    "evaluateAndShowAttention(\"c est un jeune directeur plein de talent .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.ones(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate((a, [1]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a + [4,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[0] = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-100, 2, 3, 4]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
